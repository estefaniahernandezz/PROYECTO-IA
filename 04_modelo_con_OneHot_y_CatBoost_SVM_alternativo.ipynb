{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1XrsAQKqiVTy8jVTN4rrK8PmC5uQfT8Lz",
      "authorship_tag": "ABX9TyP2s1hPZuMvxXfe1hg1JxXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/estefaniahernandezz/PROYECTO-IA/blob/main/04_modelo_con_OneHot_y_CatBoost_SVM_alternativo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxQcNwpgQ-cJ",
        "outputId": "302c0f86-3937-4160-b8ca-23a04b2b1a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño final usado para entrenar: (70000, 1041)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# =======================================================\n",
        "# 1) Cargar train_preprocesado y test\n",
        "# =======================================================\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/ARCHIVOS IA/train_preprocesado.csv\")\n",
        "test  = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# =======================================================\n",
        "# 2) Reconstruir target\n",
        "# =======================================================\n",
        "target_cols = [\n",
        "    \"RENDIMIENTO_GLOBAL_alto\",\n",
        "    \"RENDIMIENTO_GLOBAL_bajo\",\n",
        "    \"RENDIMIENTO_GLOBAL_medio-alto\",\n",
        "    \"RENDIMIENTO_GLOBAL_medio-bajo\"\n",
        "]\n",
        "\n",
        "train[\"RENDIMIENTO_GLOBAL\"] = train[target_cols].idxmax(axis=1)\n",
        "train[\"RENDIMIENTO_GLOBAL\"] = train[\"RENDIMIENTO_GLOBAL\"].str.replace(\"RENDIMIENTO_GLOBAL_\", \"\")\n",
        "\n",
        "# y = target — X = features\n",
        "y_full = train[\"RENDIMIENTO_GLOBAL\"]\n",
        "X_full = train.drop(target_cols + [\"RENDIMIENTO_GLOBAL\"], axis=1)\n",
        "\n",
        "# =======================================================\n",
        "# 3) LIMITAR CANTIDAD DE DATOS (SIN DAÑAR DISTRIBUCIÓN)\n",
        "# =======================================================\n",
        "\n",
        "LIMIT = 70000\n",
        "\n",
        "X, _, y, _ = train_test_split(\n",
        "    X_full, y_full,\n",
        "    train_size=LIMIT,\n",
        "    stratify=y_full,   # mantiene proporciones correctas\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Tamaño final usado para entrenar:\", X.shape)\n",
        "\n",
        "# =======================================================\n",
        "# 4) Concatenar para One-Hot uniforme (X + test)\n",
        "# =======================================================\n",
        "full = pd.concat([X, test], axis=0)\n",
        "\n",
        "# Completar nulos numéricos\n",
        "for c in full.select_dtypes(include=['int64','float64']).columns:\n",
        "    full[c] = full[c].fillna(full[c].median())\n",
        "\n",
        "# Completar nulos categóricos\n",
        "for c in full.select_dtypes(include=['object']).columns:\n",
        "    full[c] = full[c].fillna(\"Desconocido\")\n",
        "\n",
        "# One-hot\n",
        "full_enc = pd.get_dummies(full, drop_first=False)\n",
        "\n",
        "# Separar train y test codificados\n",
        "X_enc = full_enc.iloc[:len(X)]\n",
        "test_enc = full_enc.iloc[len(X):]\n",
        "\n",
        "# Eliminar columnas duplicadas (por seguridad)\n",
        "X_enc = X_enc.loc[:, ~X_enc.columns.duplicated()]\n",
        "test_enc = test_enc.loc[:, ~test_enc.columns.duplicated()]\n",
        "\n",
        "# Alinear (caso extremo)\n",
        "test_enc = test_enc.reindex(columns=X_enc.columns, fill_value=0)\n",
        "\n",
        "# =======================================================\n",
        "# 5) Codificar etiquetas\n",
        "# =======================================================\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "# =======================================================\n",
        "# 6) Entrenar modelo optimizado\n",
        "# =======================================================\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=20,\n",
        "    min_samples_leaf=3,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_enc, y_enc)\n",
        "\n",
        "# =======================================================\n",
        "# 7) Predicciones Kaggle\n",
        "# =======================================================\n",
        "test_pred = model.predict(test_enc)\n",
        "test_pred = le.inverse_transform(test_pred)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": test[\"ID\"],\n",
        "    \"RENDIMIENTO_GLOBAL\": test_pred\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"submission.csv generado OK.\")\n",
        "submission.head()\n"
      ]
    }
  ]
}